{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70629249-1ff7-4638-8d44-5f07e67a071b",
   "metadata": {},
   "source": [
    "# Lab 9 - Gradio and AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d259d97-5061-43af-a641-3088c4df8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f0ff9-75ff-4d81-81c7-c36a05511151",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "MODEL = \"gpt-4.1-mini\"\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc7248-ba02-4e00-9479-9435ec76721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, witty, snarky answers, no more than 1 sentence. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7beff0-eb67-4892-aa39-351d98f96d40",
   "metadata": {},
   "source": [
    "And now we wrap this in a simple chat() function,  \n",
    "and then we use the shockingly simple Gradio platform to bring up a Chatbot UI.\n",
    "\n",
    "First, the chat function, which takes the current message and the history of prior messages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf11434",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a26c37-36a3-47d9-91ad-56c7c2489a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd056244-2326-4828-93d1-4af979715017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And look how complicated it is to launch a User Interface, with the fabulous gradio platform\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba266303-707d-4e06-882a-7323bdd812db",
   "metadata": {},
   "source": [
    "## Expertise\n",
    "\n",
    "We can give our model knowledge using the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdcf2af-24ca-4d9d-a722-a6f62b623275",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \" In case it's relevant, the price of a return ticket to London is $799.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df990ea",
   "metadata": {},
   "source": [
    "#### <span style=\"color: orange;\">Question: wait - a Python question. We're changing system_message. But we already used it in the chat function above. Will the next line use the prior system_message or this new system_message? Why?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e6861-1b58-4779-b525-5925a9aea3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174aff0-b2d1-41cb-a93f-c371c8bbad76",
   "metadata": {},
   "source": [
    "## First steps towards agentic workflows\n",
    "\n",
    "Having multiple AIs collaborate to solve the problem is a simple step. Let's add another AI to the mix!\n",
    "\n",
    "Let's pick a fun one: adding multi-modality.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51107b-50ac-4d6c-aa80-205694c3bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def artist_agent(city):\n",
    "    image_response = openai.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\",\n",
    "            size=\"1024x1024\",\n",
    "            n=1,\n",
    "            response_format=\"b64_json\",\n",
    "        )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76e9be-70e5-47d7-bdc8-a484630c19a0",
   "metadata": {},
   "source": [
    "## Bringing it all together in a mini Agent Framework\n",
    "\n",
    "Bringing together both our LLM calls in 1 UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832443b-8051-426f-a787-05f6f65ec84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history):\n",
    "    message = history[-1][\"content\"]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    image =  artist_agent(\"London\") if 'london' in message.lower() else None\n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "    return history, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdcb30-dfab-4a83-958c-702522f388f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More involved Gradio code as we're not using the preset Chat interface!\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=400, type=\"messages\")\n",
    "        image_output = gr.Image(height=400)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\": \"user\", \"content\": message}]\n",
    "        return \"\", history\n",
    "\n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "        chat, inputs=chatbot, outputs=[chatbot, image_output]\n",
    "    )\n",
    "\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79b8fb-6c38-4034-b154-0a3f79e0572b",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You just created a multi-modal AI Assistant\n",
    "\n",
    "### Simple Assignment\n",
    "\n",
    "Based on this code:\n",
    "\n",
    "```python\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"sydney\": \"$2999\"}\n",
    "```\n",
    "\n",
    "Allow the Chatbot to quote ticket prices for any of those destinations, and show images!\n",
    "\n",
    "## MAJOR ASSIGNMENT\n",
    "\n",
    "Make a variation of this Chatbot that applies this to your business\n",
    "\n",
    "\n",
    "### Optional Stretch Assignment\n",
    "\n",
    "Research \"tool use\" (also known as Function Calling) and then use this technique to add Tool capabilities to your LLM to look up ticket prices.. and also have it retrieve them from a SQLLite database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee1d41",
   "metadata": {},
   "source": [
    "## Additional preparation for next week:\n",
    "\n",
    "Please gather information about yourself for our RAG example, such as resume, personal projects, any relevant artifacts.\n",
    "\n",
    "## SURVEY LINK:\n",
    "\n",
    "https://docs.google.com/forms/d/1UBoU8ItMNKZ5X0axN8LJqR0SBFDZvuhxsBIk9dwrplg/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3124ec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
